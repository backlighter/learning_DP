{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a04eb534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# 设置平台  GPU/ CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "num_workers=16\n",
    "start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义前馈神经网络模型\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14bc26",
   "metadata": {},
   "source": [
    "# 手动实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79214bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_prob=0.5 ##定义丢弃率\n",
    "def dropout(X,drop_prob):\n",
    "    X=X.float()\n",
    "    #检查丢弃概率是否在0到1之间\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1-drop_prob\n",
    "    #这种情况下吧全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    #生成mask矩阵（向量）\n",
    "    mask=(torch.rand(X.shape)<keep_prob).float()\n",
    "    mask = mask.to(X.device)\n",
    "    #按照mask进行对X进行变换\n",
    "    return mask*X/keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ab333",
   "metadata": {},
   "source": [
    "# 定义使用dropout的网络模型，两个隐藏层的丢弃率分别为0.2和0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e1f3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义前馈神经网络模型\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, is_training=True,dropout_prob=0.2):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.is_training = is_training\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        if self.is_training and self.dropout_prob > 0:  # 如果是在训练中使用dropout且丢弃概率大于0\n",
    "            out = dropout(out, drop_prob=self.dropout_prob)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a641d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 28 * 28  # 输入特征的维度\n",
    "hidden_size = 128  # 隐藏层的大小\n",
    "num_classes = 10  # 类别的数量\n",
    "learning_rate = 0.001  # 学习率\n",
    "num_epochs = 10  # 迭代次数\n",
    "batch_size = 64  # 批次大小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ede59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = MNIST(root='~/Datasets/MNIST', train=True, transform=ToTensor(), download=True)\n",
    "test_dataset = MNIST(root='~/Datasets/MNIST', train=False, transform=ToTensor(), download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,num_workers=num_workers)\n",
    "# 初始化模型\n",
    "model = FeedforwardNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40297ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c521fe4",
   "metadata": {},
   "source": [
    "## 分别实现momentum、rmsprop、adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e75cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45daa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_momentum_states(params):\n",
    "    v_w1,v_b1,v_w2,v_b2 = torch.zeros(params[0].shape),torch.zeros(params[1].shape),torch.zeros(params[2].shape),torch.zeros(params[3].shape)\n",
    "    return (v_w1,v_b1,v_w2,v_b2)\n",
    "def sgd_momentum(params,states,lr,momentum):\n",
    "    for p,v in zip(params,states):\n",
    "        with torch.no_grad():\n",
    "            v[:]=momentum*v - p.grad\n",
    "            p[:]+=lr*v\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6ed4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rmsprop_states(params):\n",
    "    s_w1, s_b1, s_w2, s_b2 = torch.zeros(params[0].shape), torch.zeros(params[1].shape), torch.zeros(params[2].shape), torch.zeros(params[3].shape)\n",
    "    return (s_w1, s_b1, s_w2, s_b2)\n",
    "\n",
    "\n",
    "def rmsprop(params, states, lr,gamma):\n",
    "    gamma, eps = gamma , 1e-6\n",
    "    for p, s in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            s[:] = gamma * s + (1 - gamma) * torch.square(p.grad)\n",
    "            p[:] -= lr * p.grad / torch.sqrt(s + eps)\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89db33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adam_states(params):\n",
    "    v_w1,v_b1,v_w2,v_b2=torch.zeros(params[0].shape),torch.zeros(params[1].shape),torch.zeros(params[2].shape),torch.zeros(params[3].shape)\n",
    "    s_w1,s_b1,s_w2,s_b2 = torch.zeros(params[0].shape),torch.zeros(params[1].shape),torch.zeros(params[2].shape),torch.zeros(params[3].shape)\n",
    "    return ((v_w1,s_w1),(v_b1,s_b1),(v_w2,s_w2),(v_b2,s_b2))\n",
    "def adam(params,states,lr,t):\n",
    "    beta1,beta2,eps = 0.9,0.999,1e-6\n",
    "    t+=1#更新迭代次数\n",
    "    \n",
    "    for p,(v,s) in zip(params,states):\n",
    "        with torch.no_grad():\n",
    "        #补全  \n",
    "            # 更新动量和二阶动量\n",
    "            v.data = beta1 * v + (1 - beta1) * p.grad.data\n",
    "            s.data = beta2 * s + (1 - beta2) * p.grad.data**2\n",
    "            # 进行偏差修正\n",
    "            v_hat = v / (1 - beta1**t)\n",
    "            s_hat = s / (1 - beta2**t)\n",
    "            # 更新参数\n",
    "            p.data -= lr * v_hat / (torch.sqrt(s_hat) + eps)\n",
    "        p.grad.data.zero_()\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf9913",
   "metadata": {},
   "source": [
    "## 使用torch.nn实现adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b7145bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.4915\n",
      "Epoch [1/10], Step [200/938], Loss: 0.3527\n",
      "Epoch [1/10], Step [300/938], Loss: 0.4273\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2500\n",
      "Epoch [1/10], Step [500/938], Loss: 0.1820\n",
      "Epoch [1/10], Step [600/938], Loss: 0.2745\n",
      "Epoch [1/10], Step [700/938], Loss: 0.2237\n",
      "Epoch [1/10], Step [800/938], Loss: 0.2364\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1363\n",
      "Test Accuracy: 0.9394\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1590\n",
      "Epoch [2/10], Step [200/938], Loss: 0.2235\n",
      "Epoch [2/10], Step [300/938], Loss: 0.1372\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1229\n",
      "Epoch [2/10], Step [500/938], Loss: 0.1700\n",
      "Epoch [2/10], Step [600/938], Loss: 0.1567\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0472\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2243\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0812\n",
      "Test Accuracy: 0.9522\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0891\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0889\n",
      "Epoch [3/10], Step [300/938], Loss: 0.1128\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0776\n",
      "Epoch [3/10], Step [500/938], Loss: 0.1792\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0896\n",
      "Epoch [3/10], Step [700/938], Loss: 0.1083\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0509\n",
      "Epoch [3/10], Step [900/938], Loss: 0.1818\n",
      "Test Accuracy: 0.9627\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0651\n",
      "Epoch [4/10], Step [200/938], Loss: 0.1668\n",
      "Epoch [4/10], Step [300/938], Loss: 0.1663\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0774\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0527\n",
      "Epoch [4/10], Step [600/938], Loss: 0.1319\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0476\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0897\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0398\n",
      "Test Accuracy: 0.9676\n",
      "Epoch [5/10], Step [100/938], Loss: 0.1636\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0384\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0502\n",
      "Epoch [5/10], Step [400/938], Loss: 0.1228\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0246\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0537\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0455\n",
      "Epoch [5/10], Step [800/938], Loss: 0.2448\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0439\n",
      "Test Accuracy: 0.9673\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0658\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0165\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0273\n",
      "Epoch [6/10], Step [400/938], Loss: 0.2175\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0763\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0667\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0701\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0410\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0631\n",
      "Test Accuracy: 0.9711\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0939\n",
      "Epoch [7/10], Step [200/938], Loss: 0.1180\n",
      "Epoch [7/10], Step [300/938], Loss: 0.1302\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0534\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0082\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0629\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0214\n",
      "Epoch [7/10], Step [800/938], Loss: 0.1054\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0396\n",
      "Test Accuracy: 0.9699\n",
      "Epoch [8/10], Step [100/938], Loss: 0.1090\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0132\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0747\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0501\n",
      "Epoch [8/10], Step [500/938], Loss: 0.1246\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0823\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0098\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0331\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0282\n",
      "Test Accuracy: 0.9690\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0561\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0375\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0943\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0278\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0102\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0827\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0177\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0206\n",
      "Epoch [9/10], Step [900/938], Loss: 0.1242\n",
      "Test Accuracy: 0.9692\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0235\n",
      "Epoch [10/10], Step [200/938], Loss: 0.1089\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0763\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0396\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0842\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0854\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0040\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0657\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0294\n",
      "Test Accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_size).to(device)\n",
    "        targets=targets.to(device)\n",
    "        # 前向传播\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 在测试集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            data = data.view(-1, input_size).to(device)\n",
    "            targets=targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a7731",
   "metadata": {},
   "source": [
    "## 使用torch.nn实现rmsprop做优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e77ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_size).to(device)\n",
    "        targets=targets.to(device)\n",
    "        # 前向传播\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 在测试集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            data = data.view(-1, input_size).to(device)\n",
    "            targets=targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00174a79",
   "metadata": {},
   "source": [
    "## 使用torch.nn实现momentum做优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87675f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/938], Loss: 2.2493\n",
      "Epoch [1/50], Step [200/938], Loss: 2.1096\n",
      "Epoch [1/50], Step [300/938], Loss: 2.0142\n",
      "Epoch [1/50], Step [400/938], Loss: 1.8844\n",
      "Epoch [1/50], Step [500/938], Loss: 1.6792\n",
      "Epoch [1/50], Step [600/938], Loss: 1.6195\n",
      "Epoch [1/50], Step [700/938], Loss: 1.5504\n",
      "Epoch [1/50], Step [800/938], Loss: 1.1844\n",
      "Epoch [1/50], Step [900/938], Loss: 1.0321\n",
      "Test Accuracy: 0.7985\n",
      "Epoch [2/50], Step [100/938], Loss: 0.9886\n",
      "Epoch [2/50], Step [200/938], Loss: 0.7797\n",
      "Epoch [2/50], Step [300/938], Loss: 0.7063\n",
      "Epoch [2/50], Step [400/938], Loss: 0.8301\n",
      "Epoch [2/50], Step [500/938], Loss: 0.7299\n",
      "Epoch [2/50], Step [600/938], Loss: 0.7665\n",
      "Epoch [2/50], Step [700/938], Loss: 0.7775\n",
      "Epoch [2/50], Step [800/938], Loss: 0.6572\n",
      "Epoch [2/50], Step [900/938], Loss: 0.8585\n",
      "Test Accuracy: 0.8606\n",
      "Epoch [3/50], Step [100/938], Loss: 0.6768\n",
      "Epoch [3/50], Step [200/938], Loss: 0.6606\n",
      "Epoch [3/50], Step [300/938], Loss: 0.4216\n",
      "Epoch [3/50], Step [400/938], Loss: 0.7141\n",
      "Epoch [3/50], Step [500/938], Loss: 0.6453\n",
      "Epoch [3/50], Step [600/938], Loss: 0.4457\n",
      "Epoch [3/50], Step [700/938], Loss: 0.6790\n",
      "Epoch [3/50], Step [800/938], Loss: 0.4636\n",
      "Epoch [3/50], Step [900/938], Loss: 0.5337\n",
      "Test Accuracy: 0.8832\n",
      "Epoch [4/50], Step [100/938], Loss: 0.3492\n",
      "Epoch [4/50], Step [200/938], Loss: 0.5330\n",
      "Epoch [4/50], Step [300/938], Loss: 0.3345\n",
      "Epoch [4/50], Step [400/938], Loss: 0.3548\n",
      "Epoch [4/50], Step [500/938], Loss: 0.4451\n",
      "Epoch [4/50], Step [600/938], Loss: 0.3657\n",
      "Epoch [4/50], Step [700/938], Loss: 0.3552\n",
      "Epoch [4/50], Step [800/938], Loss: 0.4611\n",
      "Epoch [4/50], Step [900/938], Loss: 0.2665\n",
      "Test Accuracy: 0.8934\n",
      "Epoch [5/50], Step [100/938], Loss: 0.3179\n",
      "Epoch [5/50], Step [200/938], Loss: 0.3669\n",
      "Epoch [5/50], Step [300/938], Loss: 0.4851\n",
      "Epoch [5/50], Step [400/938], Loss: 0.2130\n",
      "Epoch [5/50], Step [500/938], Loss: 0.3940\n",
      "Epoch [5/50], Step [600/938], Loss: 0.3454\n",
      "Epoch [5/50], Step [700/938], Loss: 0.2336\n",
      "Epoch [5/50], Step [800/938], Loss: 0.4796\n",
      "Epoch [5/50], Step [900/938], Loss: 0.3352\n",
      "Test Accuracy: 0.9001\n",
      "Epoch [6/50], Step [100/938], Loss: 0.5961\n",
      "Epoch [6/50], Step [200/938], Loss: 0.5351\n",
      "Epoch [6/50], Step [300/938], Loss: 0.3270\n",
      "Epoch [6/50], Step [400/938], Loss: 0.4517\n",
      "Epoch [6/50], Step [500/938], Loss: 0.4575\n",
      "Epoch [6/50], Step [600/938], Loss: 0.3110\n",
      "Epoch [6/50], Step [700/938], Loss: 0.3681\n",
      "Epoch [6/50], Step [800/938], Loss: 0.3588\n",
      "Epoch [6/50], Step [900/938], Loss: 0.3503\n",
      "Test Accuracy: 0.9038\n",
      "Epoch [7/50], Step [100/938], Loss: 0.4987\n",
      "Epoch [7/50], Step [200/938], Loss: 0.3168\n",
      "Epoch [7/50], Step [300/938], Loss: 0.4826\n",
      "Epoch [7/50], Step [400/938], Loss: 0.4777\n",
      "Epoch [7/50], Step [500/938], Loss: 0.2829\n",
      "Epoch [7/50], Step [600/938], Loss: 0.5072\n",
      "Epoch [7/50], Step [700/938], Loss: 0.3601\n",
      "Epoch [7/50], Step [800/938], Loss: 0.2822\n",
      "Epoch [7/50], Step [900/938], Loss: 0.2758\n",
      "Test Accuracy: 0.9076\n",
      "Epoch [8/50], Step [100/938], Loss: 0.5115\n",
      "Epoch [8/50], Step [200/938], Loss: 0.2939\n",
      "Epoch [8/50], Step [300/938], Loss: 0.5177\n",
      "Epoch [8/50], Step [400/938], Loss: 0.3092\n",
      "Epoch [8/50], Step [500/938], Loss: 0.4217\n",
      "Epoch [8/50], Step [600/938], Loss: 0.2989\n",
      "Epoch [8/50], Step [700/938], Loss: 0.3381\n",
      "Epoch [8/50], Step [800/938], Loss: 0.2578\n",
      "Epoch [8/50], Step [900/938], Loss: 0.3608\n",
      "Test Accuracy: 0.9104\n",
      "Epoch [9/50], Step [100/938], Loss: 0.3564\n",
      "Epoch [9/50], Step [200/938], Loss: 0.2679\n",
      "Epoch [9/50], Step [300/938], Loss: 0.3227\n",
      "Epoch [9/50], Step [400/938], Loss: 0.4537\n",
      "Epoch [9/50], Step [500/938], Loss: 0.2880\n",
      "Epoch [9/50], Step [600/938], Loss: 0.4849\n",
      "Epoch [9/50], Step [700/938], Loss: 0.2925\n",
      "Epoch [9/50], Step [800/938], Loss: 0.4556\n",
      "Epoch [9/50], Step [900/938], Loss: 0.3380\n",
      "Test Accuracy: 0.9129\n",
      "Epoch [10/50], Step [100/938], Loss: 0.3582\n",
      "Epoch [10/50], Step [200/938], Loss: 0.2190\n",
      "Epoch [10/50], Step [300/938], Loss: 0.4150\n",
      "Epoch [10/50], Step [400/938], Loss: 0.3702\n",
      "Epoch [10/50], Step [500/938], Loss: 0.3702\n",
      "Epoch [10/50], Step [600/938], Loss: 0.3796\n",
      "Epoch [10/50], Step [700/938], Loss: 0.3288\n",
      "Epoch [10/50], Step [800/938], Loss: 0.3360\n",
      "Epoch [10/50], Step [900/938], Loss: 0.1900\n",
      "Test Accuracy: 0.9155\n",
      "Epoch [11/50], Step [100/938], Loss: 0.6663\n",
      "Epoch [11/50], Step [200/938], Loss: 0.3351\n",
      "Epoch [11/50], Step [300/938], Loss: 0.4659\n",
      "Epoch [11/50], Step [400/938], Loss: 0.2569\n",
      "Epoch [11/50], Step [500/938], Loss: 0.3073\n",
      "Epoch [11/50], Step [600/938], Loss: 0.2571\n",
      "Epoch [11/50], Step [700/938], Loss: 0.4088\n",
      "Epoch [11/50], Step [800/938], Loss: 0.1386\n",
      "Epoch [11/50], Step [900/938], Loss: 0.3270\n",
      "Test Accuracy: 0.9170\n",
      "Epoch [12/50], Step [100/938], Loss: 0.1611\n",
      "Epoch [12/50], Step [200/938], Loss: 0.2262\n",
      "Epoch [12/50], Step [300/938], Loss: 0.1635\n",
      "Epoch [12/50], Step [400/938], Loss: 0.4244\n",
      "Epoch [12/50], Step [500/938], Loss: 0.1760\n",
      "Epoch [12/50], Step [600/938], Loss: 0.2580\n",
      "Epoch [12/50], Step [700/938], Loss: 0.2120\n",
      "Epoch [12/50], Step [800/938], Loss: 0.3395\n",
      "Epoch [12/50], Step [900/938], Loss: 0.2360\n",
      "Test Accuracy: 0.9184\n",
      "Epoch [13/50], Step [100/938], Loss: 0.3273\n",
      "Epoch [13/50], Step [200/938], Loss: 0.2044\n",
      "Epoch [13/50], Step [300/938], Loss: 0.3384\n",
      "Epoch [13/50], Step [400/938], Loss: 0.4374\n",
      "Epoch [13/50], Step [500/938], Loss: 0.2714\n",
      "Epoch [13/50], Step [600/938], Loss: 0.5294\n",
      "Epoch [13/50], Step [700/938], Loss: 0.3921\n",
      "Epoch [13/50], Step [800/938], Loss: 0.3811\n",
      "Epoch [13/50], Step [900/938], Loss: 0.4220\n",
      "Test Accuracy: 0.9196\n",
      "Epoch [14/50], Step [100/938], Loss: 0.1784\n",
      "Epoch [14/50], Step [200/938], Loss: 0.1887\n",
      "Epoch [14/50], Step [300/938], Loss: 0.2264\n",
      "Epoch [14/50], Step [400/938], Loss: 0.2252\n",
      "Epoch [14/50], Step [500/938], Loss: 0.2165\n",
      "Epoch [14/50], Step [600/938], Loss: 0.1752\n",
      "Epoch [14/50], Step [700/938], Loss: 0.3133\n",
      "Epoch [14/50], Step [800/938], Loss: 0.1926\n",
      "Epoch [14/50], Step [900/938], Loss: 0.2131\n",
      "Test Accuracy: 0.9226\n",
      "Epoch [15/50], Step [100/938], Loss: 0.3223\n",
      "Epoch [15/50], Step [200/938], Loss: 0.3002\n",
      "Epoch [15/50], Step [300/938], Loss: 0.1674\n",
      "Epoch [15/50], Step [400/938], Loss: 0.1384\n",
      "Epoch [15/50], Step [500/938], Loss: 0.1905\n",
      "Epoch [15/50], Step [600/938], Loss: 0.1914\n",
      "Epoch [15/50], Step [700/938], Loss: 0.3977\n",
      "Epoch [15/50], Step [800/938], Loss: 0.4084\n",
      "Epoch [15/50], Step [900/938], Loss: 0.2079\n",
      "Test Accuracy: 0.9225\n",
      "Epoch [16/50], Step [100/938], Loss: 0.1930\n",
      "Epoch [16/50], Step [200/938], Loss: 0.4554\n",
      "Epoch [16/50], Step [300/938], Loss: 0.4845\n",
      "Epoch [16/50], Step [400/938], Loss: 0.1921\n",
      "Epoch [16/50], Step [500/938], Loss: 0.2852\n",
      "Epoch [16/50], Step [600/938], Loss: 0.2077\n",
      "Epoch [16/50], Step [700/938], Loss: 0.3092\n",
      "Epoch [16/50], Step [800/938], Loss: 0.3697\n",
      "Epoch [16/50], Step [900/938], Loss: 0.1708\n",
      "Test Accuracy: 0.9238\n",
      "Epoch [17/50], Step [100/938], Loss: 0.1976\n",
      "Epoch [17/50], Step [200/938], Loss: 0.2154\n",
      "Epoch [17/50], Step [300/938], Loss: 0.1759\n",
      "Epoch [17/50], Step [400/938], Loss: 0.1337\n",
      "Epoch [17/50], Step [500/938], Loss: 0.3042\n",
      "Epoch [17/50], Step [600/938], Loss: 0.3355\n",
      "Epoch [17/50], Step [700/938], Loss: 0.2761\n",
      "Epoch [17/50], Step [800/938], Loss: 0.5036\n",
      "Epoch [17/50], Step [900/938], Loss: 0.2082\n",
      "Test Accuracy: 0.9257\n",
      "Epoch [18/50], Step [100/938], Loss: 0.2394\n",
      "Epoch [18/50], Step [200/938], Loss: 0.1086\n",
      "Epoch [18/50], Step [300/938], Loss: 0.3904\n",
      "Epoch [18/50], Step [400/938], Loss: 0.4433\n",
      "Epoch [18/50], Step [500/938], Loss: 0.1589\n",
      "Epoch [18/50], Step [600/938], Loss: 0.5370\n",
      "Epoch [18/50], Step [700/938], Loss: 0.2320\n",
      "Epoch [18/50], Step [800/938], Loss: 0.1944\n",
      "Epoch [18/50], Step [900/938], Loss: 0.1895\n",
      "Test Accuracy: 0.9265\n",
      "Epoch [19/50], Step [100/938], Loss: 0.3193\n",
      "Epoch [19/50], Step [200/938], Loss: 0.4098\n",
      "Epoch [19/50], Step [300/938], Loss: 0.3446\n",
      "Epoch [19/50], Step [400/938], Loss: 0.3056\n",
      "Epoch [19/50], Step [500/938], Loss: 0.4162\n",
      "Epoch [19/50], Step [600/938], Loss: 0.1956\n",
      "Epoch [19/50], Step [700/938], Loss: 0.2503\n",
      "Epoch [19/50], Step [800/938], Loss: 0.2083\n",
      "Epoch [19/50], Step [900/938], Loss: 0.2345\n",
      "Test Accuracy: 0.9295\n",
      "Epoch [20/50], Step [100/938], Loss: 0.2426\n",
      "Epoch [20/50], Step [200/938], Loss: 0.4148\n",
      "Epoch [20/50], Step [300/938], Loss: 0.2204\n",
      "Epoch [20/50], Step [400/938], Loss: 0.3186\n",
      "Epoch [20/50], Step [500/938], Loss: 0.3662\n",
      "Epoch [20/50], Step [600/938], Loss: 0.2173\n",
      "Epoch [20/50], Step [700/938], Loss: 0.2969\n",
      "Epoch [20/50], Step [800/938], Loss: 0.3472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Step [900/938], Loss: 0.2430\n",
      "Test Accuracy: 0.9295\n",
      "Epoch [21/50], Step [100/938], Loss: 0.2879\n",
      "Epoch [21/50], Step [200/938], Loss: 0.4706\n",
      "Epoch [21/50], Step [300/938], Loss: 0.1725\n",
      "Epoch [21/50], Step [400/938], Loss: 0.2831\n",
      "Epoch [21/50], Step [500/938], Loss: 0.3509\n",
      "Epoch [21/50], Step [600/938], Loss: 0.2923\n",
      "Epoch [21/50], Step [700/938], Loss: 0.1936\n",
      "Epoch [21/50], Step [800/938], Loss: 0.3400\n",
      "Epoch [21/50], Step [900/938], Loss: 0.1445\n",
      "Test Accuracy: 0.9303\n",
      "Epoch [22/50], Step [100/938], Loss: 0.3303\n",
      "Epoch [22/50], Step [200/938], Loss: 0.2277\n",
      "Epoch [22/50], Step [300/938], Loss: 0.2351\n",
      "Epoch [22/50], Step [400/938], Loss: 0.1011\n",
      "Epoch [22/50], Step [500/938], Loss: 0.3336\n",
      "Epoch [22/50], Step [600/938], Loss: 0.2213\n",
      "Epoch [22/50], Step [700/938], Loss: 0.4092\n",
      "Epoch [22/50], Step [800/938], Loss: 0.4772\n",
      "Epoch [22/50], Step [900/938], Loss: 0.3888\n",
      "Test Accuracy: 0.9315\n",
      "Epoch [23/50], Step [100/938], Loss: 0.3413\n",
      "Epoch [23/50], Step [200/938], Loss: 0.0878\n",
      "Epoch [23/50], Step [300/938], Loss: 0.2376\n",
      "Epoch [23/50], Step [400/938], Loss: 0.2306\n",
      "Epoch [23/50], Step [500/938], Loss: 0.3770\n",
      "Epoch [23/50], Step [600/938], Loss: 0.2241\n",
      "Epoch [23/50], Step [700/938], Loss: 0.2545\n",
      "Epoch [23/50], Step [800/938], Loss: 0.2498\n",
      "Epoch [23/50], Step [900/938], Loss: 0.1757\n",
      "Test Accuracy: 0.9334\n",
      "Epoch [24/50], Step [100/938], Loss: 0.5103\n",
      "Epoch [24/50], Step [200/938], Loss: 0.3139\n",
      "Epoch [24/50], Step [300/938], Loss: 0.1735\n",
      "Epoch [24/50], Step [400/938], Loss: 0.3071\n",
      "Epoch [24/50], Step [500/938], Loss: 0.3385\n",
      "Epoch [24/50], Step [600/938], Loss: 0.2444\n",
      "Epoch [24/50], Step [700/938], Loss: 0.4554\n",
      "Epoch [24/50], Step [800/938], Loss: 0.1054\n",
      "Epoch [24/50], Step [900/938], Loss: 0.2516\n",
      "Test Accuracy: 0.9344\n",
      "Epoch [25/50], Step [100/938], Loss: 0.1697\n",
      "Epoch [25/50], Step [200/938], Loss: 0.2228\n",
      "Epoch [25/50], Step [300/938], Loss: 0.2983\n",
      "Epoch [25/50], Step [400/938], Loss: 0.1747\n",
      "Epoch [25/50], Step [500/938], Loss: 0.1279\n",
      "Epoch [25/50], Step [600/938], Loss: 0.3054\n",
      "Epoch [25/50], Step [700/938], Loss: 0.1655\n",
      "Epoch [25/50], Step [800/938], Loss: 0.2190\n",
      "Epoch [25/50], Step [900/938], Loss: 0.3683\n",
      "Test Accuracy: 0.9347\n",
      "Epoch [26/50], Step [100/938], Loss: 0.2458\n",
      "Epoch [26/50], Step [200/938], Loss: 0.1159\n",
      "Epoch [26/50], Step [300/938], Loss: 0.2557\n",
      "Epoch [26/50], Step [400/938], Loss: 0.2410\n",
      "Epoch [26/50], Step [500/938], Loss: 0.2054\n",
      "Epoch [26/50], Step [600/938], Loss: 0.2175\n",
      "Epoch [26/50], Step [700/938], Loss: 0.1914\n",
      "Epoch [26/50], Step [800/938], Loss: 0.2199\n",
      "Epoch [26/50], Step [900/938], Loss: 0.3170\n",
      "Test Accuracy: 0.9368\n",
      "Epoch [27/50], Step [100/938], Loss: 0.3071\n",
      "Epoch [27/50], Step [200/938], Loss: 0.3924\n",
      "Epoch [27/50], Step [300/938], Loss: 0.3016\n",
      "Epoch [27/50], Step [400/938], Loss: 0.3035\n",
      "Epoch [27/50], Step [500/938], Loss: 0.0839\n",
      "Epoch [27/50], Step [600/938], Loss: 0.1877\n",
      "Epoch [27/50], Step [700/938], Loss: 0.3251\n",
      "Epoch [27/50], Step [800/938], Loss: 0.2425\n",
      "Epoch [27/50], Step [900/938], Loss: 0.2326\n",
      "Test Accuracy: 0.9364\n",
      "Epoch [28/50], Step [100/938], Loss: 0.2212\n",
      "Epoch [28/50], Step [200/938], Loss: 0.4270\n",
      "Epoch [28/50], Step [300/938], Loss: 0.2241\n",
      "Epoch [28/50], Step [400/938], Loss: 0.1251\n",
      "Epoch [28/50], Step [500/938], Loss: 0.2704\n",
      "Epoch [28/50], Step [600/938], Loss: 0.1419\n",
      "Epoch [28/50], Step [700/938], Loss: 0.2784\n",
      "Epoch [28/50], Step [800/938], Loss: 0.1353\n",
      "Epoch [28/50], Step [900/938], Loss: 0.2019\n",
      "Test Accuracy: 0.9365\n",
      "Epoch [29/50], Step [100/938], Loss: 0.1525\n",
      "Epoch [29/50], Step [200/938], Loss: 0.2558\n",
      "Epoch [29/50], Step [300/938], Loss: 0.3993\n",
      "Epoch [29/50], Step [400/938], Loss: 0.2118\n",
      "Epoch [29/50], Step [500/938], Loss: 0.2843\n",
      "Epoch [29/50], Step [600/938], Loss: 0.2929\n",
      "Epoch [29/50], Step [700/938], Loss: 0.1791\n",
      "Epoch [29/50], Step [800/938], Loss: 0.2964\n",
      "Epoch [29/50], Step [900/938], Loss: 0.1680\n",
      "Test Accuracy: 0.9383\n",
      "Epoch [30/50], Step [100/938], Loss: 0.4551\n",
      "Epoch [30/50], Step [200/938], Loss: 0.3672\n",
      "Epoch [30/50], Step [300/938], Loss: 0.0640\n",
      "Epoch [30/50], Step [400/938], Loss: 0.5923\n",
      "Epoch [30/50], Step [500/938], Loss: 0.2861\n",
      "Epoch [30/50], Step [600/938], Loss: 0.1812\n",
      "Epoch [30/50], Step [700/938], Loss: 0.1080\n",
      "Epoch [30/50], Step [800/938], Loss: 0.2136\n",
      "Epoch [30/50], Step [900/938], Loss: 0.1138\n",
      "Test Accuracy: 0.9385\n",
      "Epoch [31/50], Step [100/938], Loss: 0.1623\n",
      "Epoch [31/50], Step [200/938], Loss: 0.1275\n",
      "Epoch [31/50], Step [300/938], Loss: 0.1116\n",
      "Epoch [31/50], Step [400/938], Loss: 0.2258\n",
      "Epoch [31/50], Step [500/938], Loss: 0.1756\n",
      "Epoch [31/50], Step [600/938], Loss: 0.1939\n",
      "Epoch [31/50], Step [700/938], Loss: 0.3111\n",
      "Epoch [31/50], Step [800/938], Loss: 0.1314\n",
      "Epoch [31/50], Step [900/938], Loss: 0.2114\n",
      "Test Accuracy: 0.9400\n",
      "Epoch [32/50], Step [100/938], Loss: 0.0973\n",
      "Epoch [32/50], Step [200/938], Loss: 0.2104\n",
      "Epoch [32/50], Step [300/938], Loss: 0.2954\n",
      "Epoch [32/50], Step [400/938], Loss: 0.1955\n",
      "Epoch [32/50], Step [500/938], Loss: 0.2250\n",
      "Epoch [32/50], Step [600/938], Loss: 0.2690\n",
      "Epoch [32/50], Step [700/938], Loss: 0.1406\n",
      "Epoch [32/50], Step [800/938], Loss: 0.2816\n",
      "Epoch [32/50], Step [900/938], Loss: 0.2484\n",
      "Test Accuracy: 0.9414\n",
      "Epoch [33/50], Step [100/938], Loss: 0.2749\n",
      "Epoch [33/50], Step [200/938], Loss: 0.2669\n",
      "Epoch [33/50], Step [300/938], Loss: 0.2917\n",
      "Epoch [33/50], Step [400/938], Loss: 0.3005\n",
      "Epoch [33/50], Step [500/938], Loss: 0.3147\n",
      "Epoch [33/50], Step [600/938], Loss: 0.1778\n",
      "Epoch [33/50], Step [700/938], Loss: 0.0762\n",
      "Epoch [33/50], Step [800/938], Loss: 0.2549\n",
      "Epoch [33/50], Step [900/938], Loss: 0.3149\n",
      "Test Accuracy: 0.9412\n",
      "Epoch [34/50], Step [100/938], Loss: 0.2520\n",
      "Epoch [34/50], Step [200/938], Loss: 0.2360\n",
      "Epoch [34/50], Step [300/938], Loss: 0.0956\n",
      "Epoch [34/50], Step [400/938], Loss: 0.1732\n",
      "Epoch [34/50], Step [500/938], Loss: 0.4621\n",
      "Epoch [34/50], Step [600/938], Loss: 0.2705\n",
      "Epoch [34/50], Step [700/938], Loss: 0.1318\n",
      "Epoch [34/50], Step [800/938], Loss: 0.2035\n",
      "Epoch [34/50], Step [900/938], Loss: 0.4071\n",
      "Test Accuracy: 0.9423\n",
      "Epoch [35/50], Step [100/938], Loss: 0.1558\n",
      "Epoch [35/50], Step [200/938], Loss: 0.1798\n",
      "Epoch [35/50], Step [300/938], Loss: 0.2230\n",
      "Epoch [35/50], Step [400/938], Loss: 0.2394\n",
      "Epoch [35/50], Step [500/938], Loss: 0.2216\n",
      "Epoch [35/50], Step [600/938], Loss: 0.2644\n",
      "Epoch [35/50], Step [700/938], Loss: 0.1489\n",
      "Epoch [35/50], Step [800/938], Loss: 0.1759\n",
      "Epoch [35/50], Step [900/938], Loss: 0.4392\n",
      "Test Accuracy: 0.9421\n",
      "Epoch [36/50], Step [100/938], Loss: 0.1858\n",
      "Epoch [36/50], Step [200/938], Loss: 0.1821\n",
      "Epoch [36/50], Step [300/938], Loss: 0.2556\n",
      "Epoch [36/50], Step [400/938], Loss: 0.2668\n",
      "Epoch [36/50], Step [500/938], Loss: 0.1787\n",
      "Epoch [36/50], Step [600/938], Loss: 0.2678\n",
      "Epoch [36/50], Step [700/938], Loss: 0.3950\n",
      "Epoch [36/50], Step [800/938], Loss: 0.1477\n",
      "Epoch [36/50], Step [900/938], Loss: 0.1011\n",
      "Test Accuracy: 0.9448\n",
      "Epoch [37/50], Step [100/938], Loss: 0.3833\n",
      "Epoch [37/50], Step [200/938], Loss: 0.2668\n",
      "Epoch [37/50], Step [300/938], Loss: 0.1233\n",
      "Epoch [37/50], Step [400/938], Loss: 0.2285\n",
      "Epoch [37/50], Step [500/938], Loss: 0.2588\n",
      "Epoch [37/50], Step [600/938], Loss: 0.1304\n",
      "Epoch [37/50], Step [700/938], Loss: 0.1724\n",
      "Epoch [37/50], Step [800/938], Loss: 0.2440\n",
      "Epoch [37/50], Step [900/938], Loss: 0.3323\n",
      "Test Accuracy: 0.9444\n",
      "Epoch [38/50], Step [100/938], Loss: 0.2058\n",
      "Epoch [38/50], Step [200/938], Loss: 0.1599\n",
      "Epoch [38/50], Step [300/938], Loss: 0.1673\n",
      "Epoch [38/50], Step [400/938], Loss: 0.2768\n",
      "Epoch [38/50], Step [500/938], Loss: 0.1183\n",
      "Epoch [38/50], Step [600/938], Loss: 0.0938\n",
      "Epoch [38/50], Step [700/938], Loss: 0.2165\n",
      "Epoch [38/50], Step [800/938], Loss: 0.1403\n",
      "Epoch [38/50], Step [900/938], Loss: 0.2730\n",
      "Test Accuracy: 0.9457\n",
      "Epoch [39/50], Step [100/938], Loss: 0.2545\n",
      "Epoch [39/50], Step [200/938], Loss: 0.3763\n",
      "Epoch [39/50], Step [300/938], Loss: 0.1322\n",
      "Epoch [39/50], Step [400/938], Loss: 0.2078\n",
      "Epoch [39/50], Step [500/938], Loss: 0.3267\n",
      "Epoch [39/50], Step [600/938], Loss: 0.1407\n",
      "Epoch [39/50], Step [700/938], Loss: 0.0832\n",
      "Epoch [39/50], Step [800/938], Loss: 0.4372\n",
      "Epoch [39/50], Step [900/938], Loss: 0.0802\n",
      "Test Accuracy: 0.9462\n",
      "Epoch [40/50], Step [100/938], Loss: 0.2802\n",
      "Epoch [40/50], Step [200/938], Loss: 0.1360\n",
      "Epoch [40/50], Step [300/938], Loss: 0.2265\n",
      "Epoch [40/50], Step [400/938], Loss: 0.2107\n",
      "Epoch [40/50], Step [500/938], Loss: 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Step [600/938], Loss: 0.1674\n",
      "Epoch [40/50], Step [700/938], Loss: 0.1886\n",
      "Epoch [40/50], Step [800/938], Loss: 0.1216\n",
      "Epoch [40/50], Step [900/938], Loss: 0.1443\n",
      "Test Accuracy: 0.9472\n",
      "Epoch [41/50], Step [100/938], Loss: 0.0861\n",
      "Epoch [41/50], Step [200/938], Loss: 0.1039\n",
      "Epoch [41/50], Step [300/938], Loss: 0.1540\n",
      "Epoch [41/50], Step [400/938], Loss: 0.1482\n",
      "Epoch [41/50], Step [500/938], Loss: 0.1390\n",
      "Epoch [41/50], Step [600/938], Loss: 0.2241\n",
      "Epoch [41/50], Step [700/938], Loss: 0.1770\n",
      "Epoch [41/50], Step [800/938], Loss: 0.2754\n",
      "Epoch [41/50], Step [900/938], Loss: 0.1539\n",
      "Test Accuracy: 0.9486\n",
      "Epoch [42/50], Step [100/938], Loss: 0.1371\n",
      "Epoch [42/50], Step [200/938], Loss: 0.0990\n",
      "Epoch [42/50], Step [300/938], Loss: 0.1778\n",
      "Epoch [42/50], Step [400/938], Loss: 0.1020\n",
      "Epoch [42/50], Step [500/938], Loss: 0.2769\n",
      "Epoch [42/50], Step [600/938], Loss: 0.1611\n",
      "Epoch [42/50], Step [700/938], Loss: 0.1086\n",
      "Epoch [42/50], Step [800/938], Loss: 0.2256\n",
      "Epoch [42/50], Step [900/938], Loss: 0.2339\n",
      "Test Accuracy: 0.9489\n",
      "Epoch [43/50], Step [100/938], Loss: 0.2146\n",
      "Epoch [43/50], Step [200/938], Loss: 0.0974\n",
      "Epoch [43/50], Step [300/938], Loss: 0.1774\n",
      "Epoch [43/50], Step [400/938], Loss: 0.1278\n",
      "Epoch [43/50], Step [500/938], Loss: 0.2062\n",
      "Epoch [43/50], Step [600/938], Loss: 0.1251\n",
      "Epoch [43/50], Step [700/938], Loss: 0.2851\n",
      "Epoch [43/50], Step [800/938], Loss: 0.0795\n",
      "Epoch [43/50], Step [900/938], Loss: 0.1909\n",
      "Test Accuracy: 0.9493\n",
      "Epoch [44/50], Step [100/938], Loss: 0.1172\n",
      "Epoch [44/50], Step [200/938], Loss: 0.1272\n",
      "Epoch [44/50], Step [300/938], Loss: 0.1286\n",
      "Epoch [44/50], Step [400/938], Loss: 0.1239\n",
      "Epoch [44/50], Step [500/938], Loss: 0.1512\n",
      "Epoch [44/50], Step [600/938], Loss: 0.1760\n",
      "Epoch [44/50], Step [700/938], Loss: 0.1136\n",
      "Epoch [44/50], Step [800/938], Loss: 0.0854\n",
      "Epoch [44/50], Step [900/938], Loss: 0.2392\n",
      "Test Accuracy: 0.9497\n",
      "Epoch [45/50], Step [100/938], Loss: 0.2171\n",
      "Epoch [45/50], Step [200/938], Loss: 0.1894\n",
      "Epoch [45/50], Step [300/938], Loss: 0.1566\n",
      "Epoch [45/50], Step [400/938], Loss: 0.1418\n",
      "Epoch [45/50], Step [500/938], Loss: 0.1874\n",
      "Epoch [45/50], Step [600/938], Loss: 0.1064\n",
      "Epoch [45/50], Step [700/938], Loss: 0.2734\n",
      "Epoch [45/50], Step [800/938], Loss: 0.2287\n",
      "Epoch [45/50], Step [900/938], Loss: 0.1759\n",
      "Test Accuracy: 0.9507\n",
      "Epoch [46/50], Step [100/938], Loss: 0.1870\n",
      "Epoch [46/50], Step [200/938], Loss: 0.0659\n",
      "Epoch [46/50], Step [300/938], Loss: 0.2602\n",
      "Epoch [46/50], Step [400/938], Loss: 0.0972\n",
      "Epoch [46/50], Step [500/938], Loss: 0.2594\n",
      "Epoch [46/50], Step [600/938], Loss: 0.0960\n",
      "Epoch [46/50], Step [700/938], Loss: 0.0684\n",
      "Epoch [46/50], Step [800/938], Loss: 0.1804\n",
      "Epoch [46/50], Step [900/938], Loss: 0.1527\n",
      "Test Accuracy: 0.9510\n",
      "Epoch [47/50], Step [100/938], Loss: 0.1291\n",
      "Epoch [47/50], Step [200/938], Loss: 0.1163\n",
      "Epoch [47/50], Step [300/938], Loss: 0.1592\n",
      "Epoch [47/50], Step [400/938], Loss: 0.0898\n",
      "Epoch [47/50], Step [500/938], Loss: 0.0851\n",
      "Epoch [47/50], Step [600/938], Loss: 0.2389\n",
      "Epoch [47/50], Step [700/938], Loss: 0.1399\n",
      "Epoch [47/50], Step [800/938], Loss: 0.0635\n",
      "Epoch [47/50], Step [900/938], Loss: 0.2030\n",
      "Test Accuracy: 0.9510\n",
      "Epoch [48/50], Step [100/938], Loss: 0.2338\n",
      "Epoch [48/50], Step [200/938], Loss: 0.1879\n",
      "Epoch [48/50], Step [300/938], Loss: 0.1143\n",
      "Epoch [48/50], Step [400/938], Loss: 0.0442\n",
      "Epoch [48/50], Step [500/938], Loss: 0.1235\n",
      "Epoch [48/50], Step [600/938], Loss: 0.2471\n",
      "Epoch [48/50], Step [700/938], Loss: 0.2129\n",
      "Epoch [48/50], Step [800/938], Loss: 0.1401\n",
      "Epoch [48/50], Step [900/938], Loss: 0.1762\n",
      "Test Accuracy: 0.9522\n",
      "Epoch [49/50], Step [100/938], Loss: 0.1269\n",
      "Epoch [49/50], Step [200/938], Loss: 0.2433\n",
      "Epoch [49/50], Step [300/938], Loss: 0.1581\n",
      "Epoch [49/50], Step [400/938], Loss: 0.1778\n",
      "Epoch [49/50], Step [500/938], Loss: 0.4140\n",
      "Epoch [49/50], Step [600/938], Loss: 0.1770\n",
      "Epoch [49/50], Step [700/938], Loss: 0.1708\n",
      "Epoch [49/50], Step [800/938], Loss: 0.1425\n",
      "Epoch [49/50], Step [900/938], Loss: 0.1947\n",
      "Test Accuracy: 0.9526\n",
      "Epoch [50/50], Step [100/938], Loss: 0.1541\n",
      "Epoch [50/50], Step [200/938], Loss: 0.0532\n",
      "Epoch [50/50], Step [300/938], Loss: 0.2360\n",
      "Epoch [50/50], Step [400/938], Loss: 0.1296\n",
      "Epoch [50/50], Step [500/938], Loss: 0.0660\n",
      "Epoch [50/50], Step [600/938], Loss: 0.2926\n",
      "Epoch [50/50], Step [700/938], Loss: 0.1578\n",
      "Epoch [50/50], Step [800/938], Loss: 0.1839\n",
      "Epoch [50/50], Step [900/938], Loss: 0.0933\n",
      "Test Accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.001\n",
    "num_epochs=50\n",
    "\n",
    "momentum=0.8\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_size).to(device)\n",
    "        targets=targets.to(device)\n",
    "        # 前向传播\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # 在测试集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            data = data.view(-1, input_size).to(device)\n",
    "            targets=targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8bc8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9136\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        data = data.view(-1, input_size)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4248b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = MNIST(root='~/Datasets/MNIST', train=True, transform=ToTensor(), download=True)\n",
    "test_dataset = MNIST(root='~/Datasets/MNIST', train=False, transform=ToTensor(), download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb69e9",
   "metadata": {},
   "source": [
    "## 手动实现momentum 做优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e021dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.0205, Train Acc: 0.7039\n",
      "Epoch [2/10] - Train Loss: 0.0079, Train Acc: 0.8713\n",
      "Epoch [3/10] - Train Loss: 0.0062, Train Acc: 0.8909\n",
      "Epoch [4/10] - Train Loss: 0.0056, Train Acc: 0.8996\n",
      "Epoch [5/10] - Train Loss: 0.0052, Train Acc: 0.9058\n",
      "Epoch [6/10] - Train Loss: 0.0049, Train Acc: 0.9106\n",
      "Epoch [7/10] - Train Loss: 0.0047, Train Acc: 0.9146\n",
      "Epoch [8/10] - Train Loss: 0.0045, Train Acc: 0.9180\n",
      "Epoch [9/10] - Train Loss: 0.0043, Train Acc: 0.9219\n",
      "Epoch [10/10] - Train Loss: 0.0041, Train Acc: 0.9244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "momentum=0.9\n",
    "##定义损失函数\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params=list(model.parameters())\n",
    "states = init_momentum_states(params)\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum,train_acc_sum,n, c = 0.0,0.0,0,0\n",
    "    for X, y in train_loader:\n",
    "        # 将图像数据展平为向量\n",
    "        X = X.view(-1, input_size)\n",
    "\n",
    "        y_hat = model.forward (X)\n",
    "        l = criterion(y_hat,y).sum()\n",
    "        l.backward()\n",
    "        sgd_momentum(model.parameters(),states,learning_rate,momentum)\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "            \n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y ).sum( ).item()\n",
    "        n += y.shape[0]\n",
    "        c += 1\n",
    "        \n",
    "        # 清空梯度\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "    # 打印每个 epoch 的训练损失和准确率\n",
    "    train_loss = train_l_sum / n\n",
    "    train_acc = train_acc_sum / n\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b20b7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = MNIST(root='~/Datasets/MNIST', train=True, transform=ToTensor(), download=True)\n",
    "test_dataset = MNIST(root='~/Datasets/MNIST', train=False, transform=ToTensor(), download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1768d57",
   "metadata": {},
   "source": [
    "## 手动实现RMSprop 做优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d4a79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.0055, Train Acc: 0.9043\n",
      "Epoch [2/10] - Train Loss: 0.0026, Train Acc: 0.9523\n",
      "Epoch [3/10] - Train Loss: 0.0018, Train Acc: 0.9673\n",
      "Epoch [4/10] - Train Loss: 0.0014, Train Acc: 0.9745\n",
      "Epoch [5/10] - Train Loss: 0.0011, Train Acc: 0.9799\n",
      "Epoch [6/10] - Train Loss: 0.0009, Train Acc: 0.9824\n",
      "Epoch [7/10] - Train Loss: 0.0008, Train Acc: 0.9852\n",
      "Epoch [8/10] - Train Loss: 0.0007, Train Acc: 0.9875\n",
      "Epoch [9/10] - Train Loss: 0.0006, Train Acc: 0.9894\n",
      "Epoch [10/10] - Train Loss: 0.0005, Train Acc: 0.9907\n"
     ]
    }
   ],
   "source": [
    "###使用RMSprop优化算法\n",
    "##定义损失函数\n",
    "# 定义损失函数和优化器\n",
    "\n",
    "gamma=0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params=list(model.parameters())\n",
    "states = init_rmsprop_states(params)\n",
    "##初始化\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum,train_acc_sum,n, c = 0.0,0.0,0,0\n",
    "    for X, y in train_loader:\n",
    "        # 将图像数据展平为向量\n",
    "        X = X.view(-1, input_size)\n",
    "\n",
    "        y_hat = model.forward (X)\n",
    "        l = criterion(y_hat,y).sum()\n",
    "        l.backward()\n",
    "        rmsprop(model.parameters(),states,learning_rate,gamma)\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "            \n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y ).sum( ).item()\n",
    "        n += y.shape[0]\n",
    "        c += 1\n",
    "        \n",
    "        # 清空梯度\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "    # 打印每个 epoch 的训练损失和准确率\n",
    "    train_loss = train_l_sum / n\n",
    "    train_acc = train_acc_sum / n\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26883e",
   "metadata": {},
   "source": [
    "# 在测试集上评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "856be938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        data = data.view(-1, input_size)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aac995",
   "metadata": {},
   "source": [
    "## 手动实现Adam做优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52da1178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.0053, Train Acc: 0.9052\n",
      "Epoch [2/10] - Train Loss: 0.0033, Train Acc: 0.9403\n",
      "Epoch [3/10] - Train Loss: 0.0027, Train Acc: 0.9516\n",
      "Epoch [4/10] - Train Loss: 0.0023, Train Acc: 0.9588\n",
      "Epoch [5/10] - Train Loss: 0.0019, Train Acc: 0.9649\n",
      "Epoch [6/10] - Train Loss: 0.0017, Train Acc: 0.9700\n",
      "Epoch [7/10] - Train Loss: 0.0015, Train Acc: 0.9734\n",
      "Epoch [8/10] - Train Loss: 0.0013, Train Acc: 0.9758\n",
      "Epoch [9/10] - Train Loss: 0.0012, Train Acc: 0.9781\n",
      "Epoch [10/10] - Train Loss: 0.0011, Train Acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###使用Adam优化算法\n",
    "##定义损失函数\n",
    "# 定义损失函数和优化器\n",
    "\n",
    "t=0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params=list(model.parameters())\n",
    "states = init_adam_states(params)\n",
    "##初始化\n",
    "for epoch in range(num_epochs):\n",
    "    train_l_sum,train_acc_sum,n, c = 0.0,0.0,0,0\n",
    "    for X, y in train_loader:\n",
    "        # 将图像数据展平为向量\n",
    "        X = X.view(-1, input_size)\n",
    "\n",
    "        y_hat = model.forward (X)\n",
    "        l = criterion(y_hat,y).sum()\n",
    "        l.backward()\n",
    "        adam(model.parameters(),states,learning_rate,t)\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "            \n",
    "        train_l_sum += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y ).sum( ).item()\n",
    "        n += y.shape[0]\n",
    "        c += 1\n",
    "        \n",
    "        # 清空梯度\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.zero_()\n",
    "    # 打印每个 epoch 的训练损失和准确率\n",
    "    train_loss = train_l_sum / n\n",
    "    train_acc = train_acc_sum / n\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eba5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0122,  0.0327,  0.0321,  ...,  0.0134, -0.0317, -0.0327],\n",
       "         [-0.0354,  0.0173, -0.0207,  ...,  0.0203, -0.0192, -0.0034],\n",
       "         [ 0.0129, -0.0164, -0.0002,  ...,  0.0019, -0.0271, -0.0014],\n",
       "         ...,\n",
       "         [-0.0139,  0.0092, -0.0231,  ...,  0.0089, -0.0245,  0.0085],\n",
       "         [ 0.0206, -0.0255,  0.0088,  ..., -0.0238,  0.0314, -0.0311],\n",
       "         [-0.0239, -0.0125, -0.0284,  ...,  0.0146, -0.0071,  0.0175]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0092, -0.0109, -0.0352,  0.0090,  0.0267, -0.0016,  0.0292,  0.0075,\n",
       "         -0.0229, -0.0115,  0.0184, -0.0191, -0.0315, -0.0332, -0.0198,  0.0305,\n",
       "         -0.0140,  0.0231,  0.0070,  0.0120, -0.0244,  0.0092,  0.0350, -0.0111,\n",
       "         -0.0304, -0.0224,  0.0123,  0.0063, -0.0074,  0.0230,  0.0049,  0.0190,\n",
       "          0.0242, -0.0157,  0.0172, -0.0150,  0.0133,  0.0147,  0.0052,  0.0344,\n",
       "         -0.0252, -0.0259, -0.0036, -0.0356, -0.0063, -0.0309, -0.0224, -0.0153,\n",
       "          0.0038, -0.0284, -0.0101, -0.0056,  0.0313,  0.0228,  0.0177,  0.0246,\n",
       "         -0.0273,  0.0128,  0.0323, -0.0006,  0.0326, -0.0136, -0.0028,  0.0274,\n",
       "          0.0352, -0.0140,  0.0024,  0.0352,  0.0344,  0.0300,  0.0148, -0.0353,\n",
       "          0.0018,  0.0025,  0.0350, -0.0106, -0.0211,  0.0258,  0.0090, -0.0172,\n",
       "         -0.0355,  0.0129, -0.0286,  0.0024,  0.0031,  0.0254, -0.0167, -0.0270,\n",
       "         -0.0136,  0.0060, -0.0260,  0.0059,  0.0142, -0.0070,  0.0180, -0.0173,\n",
       "          0.0347, -0.0342,  0.0128, -0.0152, -0.0329,  0.0149, -0.0222, -0.0299,\n",
       "          0.0011,  0.0154,  0.0211,  0.0190, -0.0064,  0.0093, -0.0248, -0.0160,\n",
       "         -0.0247,  0.0186,  0.0140,  0.0128,  0.0063,  0.0202,  0.0333, -0.0078,\n",
       "         -0.0282,  0.0143, -0.0067,  0.0319,  0.0066, -0.0244,  0.0269, -0.0085],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0158,  0.0122, -0.0850,  ...,  0.0626,  0.0223, -0.0168],\n",
       "         [-0.0587, -0.0143,  0.0225,  ..., -0.0466, -0.0510, -0.0318],\n",
       "         [-0.0760, -0.0488,  0.0414,  ...,  0.0606,  0.0848, -0.0628],\n",
       "         ...,\n",
       "         [ 0.0681, -0.0366,  0.0685,  ..., -0.0851, -0.0273, -0.0749],\n",
       "         [-0.0104, -0.0111,  0.0759,  ..., -0.0286, -0.0065,  0.0336],\n",
       "         [-0.0257,  0.0075,  0.0536,  ..., -0.0833,  0.0878,  0.0068]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0830, -0.0813, -0.0743,  0.0218,  0.0402,  0.0379, -0.0763,  0.0785,\n",
       "          0.0814, -0.0144], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffd8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'multi_class_model2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d01f2",
   "metadata": {},
   "source": [
    "# 在多分类任务实验中分别手动和用torch.nn实现L2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864156c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeedforwardNN' object has no attribute 'dropout_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 添加 L2 正则化项\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda_11/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mFeedforwardNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 如果是在训练中使用dropout且丢弃概率大于0\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m dropout(out, drop_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob)\n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuda_11/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeedforwardNN' object has no attribute 'dropout_prob'"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类和模型类，请根据你的数据和模型进行相应的定义\n",
    "\n",
    "# 定义手动实现 L2 正则化的损失函数\n",
    "def manual_l2_loss(model, l2_lambda):\n",
    "    l2_reg = 0.0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.sum(param**2)\n",
    "    return l2_lambda * l2_reg\n",
    "\n",
    "# 其他代码不变，只修改损失函数的计算和反向传播部分\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "wd = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_size).to(device)\n",
    "        targets = targets.to(device)\n",
    "        # 前向传播\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 添加 L2 正则化项\n",
    "        loss += manual_l2_loss(model, wd)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # 在训练集上计算准确率并记录\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in train_loader:\n",
    "            data = data.view(-1, input_size).to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # 在测试集上评估模型并计算准确率并记录\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            data = data.view(-1, input_size).to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        test_accuracy = correct / total\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"模型的training_time为\" + str(round(training_time, 2)) + \"秒\")\n",
    "\n",
    "# 绘制训练集和测试集准确率图表\n",
    "plt.figure(figsize=(8, 6))\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, train_accuracy_list, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs, test_accuracy_list, label='Test Accuracy', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6744952",
   "metadata": {},
   "source": [
    "## 以前馈神经网络为例，当使用dropout时，前馈神经网络隐藏层中的隐藏单元hi有一定概率被丢弃掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67a4fa",
   "metadata": {},
   "source": [
    "# 设丢弃概率为p，那么有p的概率hi会被清0，有1-p的概率hi会被除以1-p做拉伸。由此定义进行dropout操作的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8342a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X,drop_prob):\n",
    "    X=X.float()\n",
    "    #检查丢弃概率是否在0到1之间\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1-drop_prob\n",
    "    #这种情况下吧全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    #生成mask矩阵（向量）\n",
    "    mask=(torch.rand(X.shape)<keep_prob).float()\n",
    "    #按照mask进行对X进行变换\n",
    "    return mask*X/keep_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0558bc",
   "metadata": {},
   "source": [
    "# 初始化一个向量X，对X进行dropout,分别设置丢弃率为0、0.5、1.实验结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84edb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.arange(10).view(2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438e0e3",
   "metadata": {},
   "source": [
    "# 快捷键P+M可以把单元格切换成markdown的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09498d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8., 9.]]) \n",
      "\n",
      "tensor([[ 0.,  2.,  0.,  6.,  8.],\n",
      "        [ 0., 12.,  0., 16.,  0.]]) \n",
      "\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dropout(X,0),'\\n')\n",
    "print(dropout(X,0.5),'\\n')\n",
    "print(dropout(X,1),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c97404",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1,drop_prob2=0.2,0.5\n",
    "\n",
    "def net(X,is_training=True):\n",
    "    X=X.view(-1,num_inputs)\n",
    "    H1 = (torch.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
