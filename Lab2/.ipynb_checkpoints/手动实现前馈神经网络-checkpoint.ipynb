{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390eec18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m train_labels, test_labels \u001b[38;5;241m=\u001b[39m labels[:n_train], labels[n_train:]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# # 定义线性回归模型\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# def linear_regression(X, w, b):\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     return torch.matmul(X, w) + b\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#定义前馈神经网络\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 定义前馈神经网络模型\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFeedforwardNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size1, hidden_size2, output_size):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28msuper\u001b[39m(FeedforwardNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "# 设置参数和数据\n",
    "num_inputs = 500\n",
    "n_train = 7000\n",
    "n_test = 3000\n",
    "true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05\n",
    "features = torch.randn((n_train + n_test, num_inputs))\n",
    "labels = torch.matmul(features, true_w) + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "train_features, test_features = features[:n_train, :], features[n_train:, :]\n",
    "train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "\n",
    "# # 定义线性回归模型\n",
    "# def linear_regression(X, w, b):\n",
    "#     return torch.matmul(X, w) + b\n",
    "#定义前馈神经网络\n",
    "# 定义前馈神经网络模型\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 定义模型参数并将其添加为nn.Parameter\n",
    "        self.W1 = nn.Parameter(torch.randn(input_size, hidden_size1))\n",
    "        self.b1 = nn.Parameter(torch.zeros(hidden_size1))\n",
    "        self.W2 = nn.Parameter(torch.randn(hidden_size1, hidden_size2))\n",
    "        self.b2 = nn.Parameter(torch.zeros(hidden_size2))\n",
    "        self.W3 = nn.Parameter(torch.randn(hidden_size2, output_size))\n",
    "        self.b3 = nn.Parameter(torch.zeros(output_size))\n",
    "        # 初始化权重和偏置\n",
    "        self.W1 = torch.randn(self.input_size, self.hidden_size1, requires_grad=True)\n",
    "        self.b1 = torch.zeros(self.hidden_size1, requires_grad=True)\n",
    "        self.W2 = torch.randn(self.hidden_size1, self.hidden_size2, requires_grad=True)\n",
    "        self.b2 = torch.zeros(self.hidden_size2, requires_grad=True)\n",
    "        self.W3 = torch.randn(self.hidden_size2, self.output_size, requires_grad=True)\n",
    "        self.b3 = torch.zeros(self.output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 第一个隐藏层\n",
    "        h1 = torch.relu(torch.matmul(x, self.W1) + self.b1)\n",
    "        # 第二个隐藏层\n",
    "        h2 = torch.relu(torch.matmul(h1, self.W2) + self.b2)\n",
    "        # 输出层\n",
    "        out = torch.matmul(h2, self.W3) + self.b3\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# 定义随机梯度下降优化算法\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param -= lr * param.grad / batch_size\n",
    "                param.grad.zero_()\n",
    "\n",
    "# 初始化模型参数\n",
    "w = torch.randn(num_inputs, 1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "model=FeedforwardNN(num_inputs,128, 64,1)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        X = train_features[i:i+batch_size]\n",
    "        y = train_labels[i:i+batch_size]\n",
    "\n",
    "        # 前向传播\n",
    "        y_pred = model.forward(X)\n",
    "        loss = mean_squared_error(y_pred, y)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 使用SGD优化算法更新参数\n",
    "        sgd(model.parameters(), lr, batch_size)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "with torch.no_grad():\n",
    "    test_outputs =model.forward(test_features)\n",
    "    test_loss = mean_squared_error(test_outputs, test_labels)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5096285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: nan\n",
      "Epoch [20/100], Loss: nan\n",
      "Epoch [30/100], Loss: nan\n",
      "Epoch [40/100], Loss: nan\n",
      "Epoch [50/100], Loss: nan\n",
      "Epoch [60/100], Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m=\u001b[39mmean_squared_error(outputs,train_labels)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39mW1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mW1\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l-zh/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l-zh/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义前馈神经网络模型\n",
    "class FeedforwardNN:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 初始化权重和偏置\n",
    "        self.W1 = torch.randn(self.input_size, self.hidden_size1, requires_grad=True)\n",
    "        self.b1 = torch.zeros(self.hidden_size1, requires_grad=True)\n",
    "        self.W2 = torch.randn(self.hidden_size1, self.hidden_size2, requires_grad=True)\n",
    "        self.b2 = torch.zeros(self.hidden_size2, requires_grad=True)\n",
    "        self.W3 = torch.randn(self.hidden_size2, self.output_size, requires_grad=True)\n",
    "        self.b3 = torch.zeros(self.output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 第一个隐藏层\n",
    "        h1 = torch.relu(torch.matmul(x, self.W1) + self.b1)\n",
    "        # 第二个隐藏层\n",
    "        h2 = torch.relu(torch.matmul(h1, self.W2) + self.b2)\n",
    "        # 输出层\n",
    "        out = torch.matmul(h2, self.W3) + self.b3\n",
    "        return out\n",
    "\n",
    "# 定义均方误差损失函数\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    return ((y_pred - y_true)**2).mean()    \n",
    "    \n",
    "# 设置参数和数据\n",
    "num_inputs = 500\n",
    "n_train = 7000\n",
    "n_test = 3000\n",
    "true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05\n",
    "features = torch.randn((n_train + n_test, num_inputs))\n",
    "labels = torch.matmul(features, true_w) + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "train_features, test_features = features[:n_train, :], features[n_train:, :]\n",
    "train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "\n",
    "# 初始化模型\n",
    "model = FeedforwardNN(input_size=num_inputs, hidden_size1=100, hidden_size2=100, output_size=1)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "learning_rate = 0.03\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    outputs = model.forward(train_features)\n",
    "    loss=mean_squared_error(outputs,train_labels)\n",
    "    \n",
    "    \n",
    "    # 反向传播和优化\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        model.W1 -= learning_rate * model.W1.grad\n",
    "        model.b1 -= learning_rate * model.b1.grad\n",
    "        model.W2 -= learning_rate * model.W2.grad\n",
    "        model.b2 -= learning_rate * model.b2.grad\n",
    "        model.W3 -= learning_rate * model.W3.grad\n",
    "        model.b3 -= learning_rate * model.b3.grad\n",
    "        # 梯度清零\n",
    "        model.W1.grad.zero_()\n",
    "        model.b1.grad.zero_()\n",
    "        model.W2.grad.zero_()\n",
    "        model.b2.grad.zero_()\n",
    "        model.W3.grad.zero_()\n",
    "        model.b3.grad.zero_()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "with torch.no_grad():\n",
    "    test_outputs = model.forward(test_features)\n",
    "    test_loss = ((test_outputs - test_labels)**2).mean()\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8de21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
